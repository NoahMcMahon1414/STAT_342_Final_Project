---
title: "Final Project Code"
subtitle:  "Cheeson Lau, Edison Lu, and Noah McMahon"
graphics: yes
output: pdf_document
header-includes:
    - \usepackage{amsmath, amssymb}
    - \usepackage{framed}\definecolor{shadecolor}{rgb}{0.949,0.949,0.949}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
```

### Creating the Dataset

```{r Creating the Dataset}

cov <- tibble("Group" = c("BNT162b2", "Placebo", "Total"), 
              "COVID-19 Cases" = c(8, 162, 170), 
              "No. of Subjects" = c(17411, 17511, 34922))

```

### Checking Values from Paper

We know that P(COVID | BNT162b2) = $\pi_v$, and that P(COVID | Placebo) = $\pi_p$.

```{r pi_v and pi_p}

pi_v <- (8 / 17411)
pi_p <- (162 / 17511)

```

Looking at $T \sim Binom(n = 170, \pi)$, we know that $\pi = \frac{\pi_v}{\pi_v + \pi_p}$ from the instructions.
Additionally, the vaccine efficacy $\psi = \frac{1 - 2\pi}{1 - \pi}$.

```{r pi and psi}

pi <- (pi_v) / (pi_v + pi_p)
psi <- (1 - (2 * pi)) / (1 - pi)

```

### Maximum Likelihood Estimator

We know that $T \sim Binom(n = 170, \pi)$ and that we have observed t = 8. Additionally, the parameter of interest is the vaccine efficacy $\psi = \frac{1 - 2\pi}{1 - \pi}$.

Using the Invariance of the MLE (which we proved in Problem 6), we can say that
\begin{center}
$\hat{\psi_0}^{MLE} = \frac{1 - 2\hat{\pi_0}^{MLE}}{1 - \hat{\pi_0}^{MLE}}$
\end{center}

### Method of Moments Estimator

We know that $T \sim Binom(n = 170, \pi)$ and that we have observed t = 8. Additionally, the parameter of interest is the vaccine efficacy $\psi = \frac{1 - 2\pi}{1 - \pi}$. We know that $\pi = \frac{1 - \psi}{2 - \psi}$.

The method of moments estimator satisfies
\begin{align}
E[T] &= \bar{t} \\
170 * \pi &= \frac{8}{1} \\
170 * \frac{1 - \psi}{2 - \psi} &= 8 \\
\frac{1 - \psi}{2 - \psi} &= \frac{8}{170} \\
1 - \psi &= \frac{16}{170} - \frac{8}{170}\psi \\
\frac{154}{170} &= \frac{162}{170}\psi \\
154 &= 162\psi \\
\hat{\psi_0}^{MOM} &= \frac{154}{162}
\end{align}

We must now verify that $\hat{\psi_0}^{MOM} = \frac{n - 2T}{n - T}$.
\begin{align}
E[T] &= \bar{t} \\
n * \pi &= t \\
n * \frac{1 - \psi}{2 - \psi} &= t \\
\frac{n - n\psi}{2 - \psi} &= t \\
n - n\psi &= 2t - t\psi \\
n - 2t &= n\psi + t\psi \\
\psi(n + t) &= n - 2t \\
\psi &= \frac{n - 2t}{n + t}
\end{align}

Now that we have verified that $\hat{\psi_0}^{MOM} = \frac{n - 2T}{n - T}$, we can use the parametric bootstrap to find a confidence interval for $\hat{\psi_0}^{MOM}$ since we know the distribution is Binomial.

```{r Parametric Bootstrap CI MOM}

pi_mom_der <- (154/162)

B = 1000
set.seed(8383)
boot_df <- tibble(
  psi_star = replicate(n = B,
                       expr = ((170 - (2 * rbinom(n = 15, size = 170, prob = pi_mom_der))) / (170 - rbinom(n = 15, size = 170, prob = pi_mom_der))) )
)

ggplot(data = boot_df,
       mapping = aes(x = psi_star) ) +
  geom_histogram(binwidth = 0.3) +
  labs(title = "Bootstrapped Sampling Distribution of ",
       subtitle = expression(hat(psi)[0]^{MOM} == frac((n - (2 * T)),(n - T))),
       x = expression(frac((n - (2 * T)),(n - T))),
       y = "Count")

boot_df %>% summarise(boot_mean = mean(psi_star), boot_sd = sd(psi_star))

boot_mean <- mean(boot_df$psi_star)
boot_sd <- sd(boot_df$psi_star)

lower_mom <- boot_mean - (1.96 * boot_sd)
upper_mom <- boot_mean + (1.96 * boot_sd)

```

Thus, the 95\% confidence interval for $\hat{\psi_0}^{MOM}$, achieved through parametric bootstrapping, is [`r round(lower_mom, 4)`, `r round(upper_mom, 4)`].
