---
title: "Final Project Code"
subtitle:  "Cheeson Lau, Edison Lu, and Noah McMahon"
graphics: yes
output: pdf_document
header-includes:
    - \usepackage{amsmath, amssymb}
    - \usepackage{framed}\definecolor{shadecolor}{rgb}{0.949,0.949,0.949}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
```

### Creating the Dataset

```{r Creating the Dataset}

cov <- tibble("Group" = c("BNT162b2", "Placebo", "Total"), 
              "COVID-19 Cases" = c(8, 162, 170), 
              "No. of Subjects" = c(17411, 17511, 34922))

```

### Checking Values from Paper

We know that P(COVID | BNT162b2) = $\pi_v$, and that P(COVID | Placebo) = $\pi_p$.

```{r pi_v and pi_p}

pi_v <- (8 / 17411)
pi_p <- (162 / 17511)

```

Looking at $T \sim Binom(n = 170, \pi)$, we know that $\pi = \frac{\pi_v}{\pi_v + \pi_p}$ from the instructions.
Additionally, the vaccine efficacy $\psi = \frac{1 - 2\pi}{1 - \pi}$.

```{r pi and psi}

pi <- (pi_v) / (pi_v + pi_p)
psi <- (1 - (2 * pi)) / (1 - pi)

```

### Maximum Likelihood Estimator

We know that $T \sim Binom(n = 170, \pi)$ and that we have observed t = 8. Additionally, the parameter of interest is the vaccine efficacy $\psi = \frac{1 - 2\pi}{1 - \pi}$.

Using the Invariance of the MLE (which we proved in Problem 6), we can say that
\begin{center}
$\hat{\psi_0}^{MLE} = \frac{1 - 2\hat{\pi_0}^{MLE}}{1 - \hat{\pi_0}^{MLE}}$
\end{center}

### Method of Moments Estimator

We know that $T \sim Binom(n = 170, \pi)$ and that we have observed t = 8. Additionally, the parameter of interest is the vaccine efficacy $\psi = \frac{1 - 2\pi}{1 - \pi}$. We know that $\pi = \frac{1 - \psi}{2 - \psi}$.

The method of moments estimator satisfies
\begin{align*}
E[T] &= \bar{t} \\
170 * \pi &= 8 \\
170 * \frac{1 - \psi}{2 - \psi} &= 8 \\
\frac{1 - \psi}{2 - \psi} &= \frac{8}{170} \\
1 - \psi &= \frac{16}{170} - \frac{8}{170}\psi \\
\frac{154}{170} &= \frac{162}{170}\psi \\
154 &= 162\psi \\
\hat{\psi_0}^{MOM} &= \frac{154}{162}
\end{align*}

We must now verify that $\hat{\psi_0}^{MOM} = \frac{n - 2T}{n - T}$.
\begin{align*}
E[T] &= \bar{t} \\
n * \pi &= t \\
n * \frac{1 - \psi}{2 - \psi} &= t \\
\frac{n - n\psi}{2 - \psi} &= t \\
n - n\psi &= 2t - t\psi \\
n - 2t &= n\psi - t\psi \\
\psi(n - t) &= n - 2t \\
\psi &= \frac{n - 2t}{n - t}\\
\end{align*}

From above, we can also see that
\begin{align*}
\hat{\pi_0}^{MOM} = \frac{t}{n} = \frac{8}{170}
\end{align*}

Now that we have verified that $\hat{\psi_0}^{MOM} = \frac{n - 2T}{n - T}$, we can use the parametric bootstrap to find a confidence interval for $\hat{\psi_0}^{MOM}$ since we know the distribution is Binomial.

```{r Parametric Bootstrap CI MOM}
pi_mom <- 8/170
psi_mom <- 154/162

#generate sample of n = 15 from Binom(170, 8) and then calculate new estimate of psi_mom. Repeat 1000 times
B = 1000
set.seed(8383)
boot_df <- tibble(
  psi_star = replicate(n = B,
                       {sample <- rbinom(n = 15, size = 170, prob = pi_mom)
                        sample_mean <- mean(sample)
                        (170 - 2 * sample_mean) / (170 - sample_mean)})
)

#make a histogram of the bootstrap estimates
ggplot(data = boot_df,
       mapping = aes(x = psi_star) ) +
  geom_histogram(bins = 10) +
  labs(title = "Bootstrapped Sampling Distribution of ",
       subtitle = expression(hat(psi)[0]^{MOM} == frac(n - 2 * T,n - T)),
       x = expression(hat(psi)[0]^{MOM}),
       y = "Count")

boot_mean <- mean(boot_df$psi_star)
boot_se <- sd(boot_df$psi_star)
bias <- boot_mean - psi_mom
bias_corrected_estimate <- psi_mom - bias
lower_mom <- bias_corrected_estimate - qnorm(0.975) * boot_se
upper_mom <- bias_corrected_estimate + qnorm(0.975) * boot_se

boot_df %>% summarise(boot_mean = round(boot_mean, 6), 
                      boot_se = round(boot_se, 6),
                      bias = round(bias, 10),
                      bias_corrected_estimate = round(bias_corrected_estimate, 6),
                      lower = round(lower_mom, 4), 
                      upper = round(upper_mom, 4))
```

Thus, the 95% confidence interval for $\hat{\psi_0}^{MOM}$, achieved through parametric bootstrapping, is [`r round(lower_mom, 4)`, `r round(upper_mom, 4)`].
